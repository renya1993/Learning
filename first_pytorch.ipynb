{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSgbQ+kPNmGC7yRrjTjRQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renya1993/Learning/blob/main/first_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQO6o8yNceeg"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizers"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "914HPPt-eu-q"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,input_dim,hidden_dim,output_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(input_dim,hidden_dim)\n",
        "    self.a1 = nn.Sigmoid()\n",
        "    self.l2 = nn.Linear(hidden_dim,output_dim)\n",
        "    self.a2 = nn.Sigmoid()\n",
        "\n",
        "    self.layers = [self.l1,self.a1,self.l2,self.a2]\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4lgxiAWeOsn"
      },
      "source": [
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MLP(2,3,1).to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6gNJ42lepV3"
      },
      "source": [
        "N=300\n",
        "x,t = datasets.make_moons(N,noise=0.3)\n",
        "t = t.reshape(N,1)\n",
        "\n",
        "x_train,x_test,t_train,t_test = train_test_split(x,t,test_size=0.2)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8hRzxG6fYAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a80fa5c-b2a4-4231-aea3-bab5c83d186a"
      },
      "source": [
        "## 2クラスの誤差はこれで求める　中身は、E=1/N*tlogy +(1-t)*log(1-y)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "## model.parameters()が更新したいパラメータ、t,x\n",
        "optimizer = optimizers.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "def compute_loss(t,y):\n",
        "  return criterion(y,t)\n",
        "\n",
        "def train_step(x,t):\n",
        "  model.train() #訓練モード\n",
        "  preds = model(x) \n",
        "  loss = compute_loss(t,preds) #これが最小になった時、いいw,bが手に入る。\n",
        "  optimizer.zero_grad() #勾配の初期化 :pytorchでは、勾配が勝手に足されていくので、それを避けるため。前回の分が計算に入っちゃう。\n",
        "  loss.backward() #損失関数E=1/N*tlogy +(1-t)*log(1-y)のx,tについて微分を計算。\n",
        "  optimizer.step() #w←de/dwの計算\n",
        "  return loss\n",
        "\n",
        "epochs = 300\n",
        "batch_size = 10\n",
        "n_batches = x_train.shape[0]//batch_size\n",
        "\n",
        "\"\"\"\n",
        "学習\n",
        "\"\"\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"------------\",epoch+1,\"---------------\")\n",
        "  train_loss = 0\n",
        "  x_,t_ = shuffle(x_train,t_train)\n",
        "  x_ = torch.Tensor(x_).to(device)\n",
        "  t_ = torch.Tensor(t_).to(device)\n",
        "\n",
        "  for n_batch in range(n_batches):\n",
        "    start = n_batch*batch_size\n",
        "    end = start + batch_size\n",
        "    loss = train_step(x_[start:end],t_[start:end])\n",
        "    train_loss += loss.item() #損失関数は、Σ全データなので。こう言う処理をする。フルスクラッチの場合.sum(axis=1)とかで対処したりする。\n",
        "\n",
        "  print('epoch: {}, loss: {:.3}'.format(\n",
        "      epoch+1,\n",
        "      train_loss\n",
        "  ))\n",
        "\"\"\"\n",
        "モデルの評価\n",
        "\"\"\"\n",
        "\n",
        "def test_step(x,t):\n",
        "  x = torch.Tensor(x).to(device)\n",
        "  t = torch.Tensor(t).to(device)\n",
        "  model.eval()\n",
        "  preds = model(x)\n",
        "  loss = compute_loss(t,preds)\n",
        "  return loss,preds\n",
        "\n",
        "loss,preds = test_step(x_test,t_test)\n",
        "test_loss = loss.item()\n",
        "preds = preds.data.cpu().numpy() >0.5 #0.5より大きい時は、True=1、0.5より小さければfalse=0\n",
        "\n",
        "\"\"\"\n",
        "preds :テンソル型\n",
        "preds.data 勾配情報を含まないテンソル型\n",
        "preds.data.cpu() CPUのテンソル形\n",
        "preds.data.cpu().numpy()  Numpy配列\n",
        "\"\"\"\n",
        "\n",
        "test_acc = accuracy_score(t_test,preds)\n",
        "print(test_loss,test_acc)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 5.96\n",
            "epoch: 2, loss: 5.95\n",
            "epoch: 3, loss: 5.93\n",
            "epoch: 4, loss: 5.95\n",
            "epoch: 5, loss: 5.95\n",
            "epoch: 6, loss: 5.95\n",
            "epoch: 7, loss: 5.94\n",
            "epoch: 8, loss: 5.94\n",
            "epoch: 9, loss: 5.95\n",
            "epoch: 10, loss: 5.96\n",
            "epoch: 11, loss: 5.94\n",
            "epoch: 12, loss: 5.96\n",
            "epoch: 13, loss: 5.96\n",
            "epoch: 14, loss: 5.96\n",
            "epoch: 15, loss: 5.93\n",
            "epoch: 16, loss: 5.95\n",
            "epoch: 17, loss: 5.96\n",
            "epoch: 18, loss: 5.95\n",
            "epoch: 19, loss: 5.94\n",
            "epoch: 20, loss: 5.94\n",
            "epoch: 21, loss: 5.95\n",
            "epoch: 22, loss: 5.95\n",
            "epoch: 23, loss: 5.93\n",
            "epoch: 24, loss: 5.94\n",
            "epoch: 25, loss: 5.94\n",
            "epoch: 26, loss: 5.93\n",
            "epoch: 27, loss: 5.97\n",
            "epoch: 28, loss: 5.93\n",
            "epoch: 29, loss: 5.94\n",
            "epoch: 30, loss: 5.94\n",
            "epoch: 31, loss: 5.94\n",
            "epoch: 32, loss: 5.94\n",
            "epoch: 33, loss: 5.92\n",
            "epoch: 34, loss: 5.92\n",
            "epoch: 35, loss: 5.94\n",
            "epoch: 36, loss: 5.94\n",
            "epoch: 37, loss: 5.94\n",
            "epoch: 38, loss: 5.91\n",
            "epoch: 39, loss: 5.93\n",
            "epoch: 40, loss: 5.94\n",
            "epoch: 41, loss: 5.94\n",
            "epoch: 42, loss: 5.94\n",
            "epoch: 43, loss: 5.93\n",
            "epoch: 44, loss: 5.97\n",
            "epoch: 45, loss: 5.94\n",
            "epoch: 46, loss: 5.92\n",
            "epoch: 47, loss: 5.94\n",
            "epoch: 48, loss: 5.94\n",
            "epoch: 49, loss: 5.94\n",
            "epoch: 50, loss: 5.93\n",
            "epoch: 51, loss: 5.93\n",
            "epoch: 52, loss: 5.95\n",
            "epoch: 53, loss: 5.95\n",
            "epoch: 54, loss: 5.94\n",
            "epoch: 55, loss: 5.94\n",
            "epoch: 56, loss: 5.95\n",
            "epoch: 57, loss: 5.95\n",
            "epoch: 58, loss: 5.92\n",
            "epoch: 59, loss: 5.94\n",
            "epoch: 60, loss: 5.93\n",
            "epoch: 61, loss: 5.94\n",
            "epoch: 62, loss: 5.93\n",
            "epoch: 63, loss: 5.92\n",
            "epoch: 64, loss: 5.92\n",
            "epoch: 65, loss: 5.92\n",
            "epoch: 66, loss: 5.94\n",
            "epoch: 67, loss: 5.93\n",
            "epoch: 68, loss: 5.93\n",
            "epoch: 69, loss: 5.92\n",
            "epoch: 70, loss: 5.94\n",
            "epoch: 71, loss: 5.92\n",
            "epoch: 72, loss: 5.96\n",
            "epoch: 73, loss: 5.92\n",
            "epoch: 74, loss: 5.93\n",
            "epoch: 75, loss: 5.92\n",
            "epoch: 76, loss: 5.92\n",
            "epoch: 77, loss: 5.93\n",
            "epoch: 78, loss: 5.92\n",
            "epoch: 79, loss: 5.93\n",
            "epoch: 80, loss: 5.95\n",
            "epoch: 81, loss: 5.92\n",
            "epoch: 82, loss: 5.91\n",
            "epoch: 83, loss: 5.93\n",
            "epoch: 84, loss: 5.94\n",
            "epoch: 85, loss: 5.92\n",
            "epoch: 86, loss: 5.92\n",
            "epoch: 87, loss: 5.91\n",
            "epoch: 88, loss: 5.92\n",
            "epoch: 89, loss: 5.91\n",
            "epoch: 90, loss: 5.92\n",
            "epoch: 91, loss: 5.91\n",
            "epoch: 92, loss: 5.91\n",
            "epoch: 93, loss: 5.93\n",
            "epoch: 94, loss: 5.91\n",
            "epoch: 95, loss: 5.92\n",
            "epoch: 96, loss: 5.92\n",
            "epoch: 97, loss: 5.92\n",
            "epoch: 98, loss: 5.91\n",
            "epoch: 99, loss: 5.92\n",
            "epoch: 100, loss: 5.92\n",
            "epoch: 101, loss: 5.91\n",
            "epoch: 102, loss: 5.92\n",
            "epoch: 103, loss: 5.9\n",
            "epoch: 104, loss: 5.93\n",
            "epoch: 105, loss: 5.9\n",
            "epoch: 106, loss: 5.9\n",
            "epoch: 107, loss: 5.91\n",
            "epoch: 108, loss: 5.92\n",
            "epoch: 109, loss: 5.92\n",
            "epoch: 110, loss: 5.91\n",
            "epoch: 111, loss: 5.91\n",
            "epoch: 112, loss: 5.9\n",
            "epoch: 113, loss: 5.91\n",
            "epoch: 114, loss: 5.9\n",
            "epoch: 115, loss: 5.91\n",
            "epoch: 116, loss: 5.93\n",
            "epoch: 117, loss: 5.92\n",
            "epoch: 118, loss: 5.9\n",
            "epoch: 119, loss: 5.9\n",
            "epoch: 120, loss: 5.89\n",
            "epoch: 121, loss: 5.92\n",
            "epoch: 122, loss: 5.92\n",
            "epoch: 123, loss: 5.92\n",
            "epoch: 124, loss: 5.9\n",
            "epoch: 125, loss: 5.91\n",
            "epoch: 126, loss: 5.91\n",
            "epoch: 127, loss: 5.89\n",
            "epoch: 128, loss: 5.9\n",
            "epoch: 129, loss: 5.92\n",
            "epoch: 130, loss: 5.92\n",
            "epoch: 131, loss: 5.9\n",
            "epoch: 132, loss: 5.9\n",
            "epoch: 133, loss: 5.91\n",
            "epoch: 134, loss: 5.9\n",
            "epoch: 135, loss: 5.92\n",
            "epoch: 136, loss: 5.91\n",
            "epoch: 137, loss: 5.91\n",
            "epoch: 138, loss: 5.89\n",
            "epoch: 139, loss: 5.91\n",
            "epoch: 140, loss: 5.91\n",
            "epoch: 141, loss: 5.9\n",
            "epoch: 142, loss: 5.91\n",
            "epoch: 143, loss: 5.89\n",
            "epoch: 144, loss: 5.9\n",
            "epoch: 145, loss: 5.89\n",
            "epoch: 146, loss: 5.9\n",
            "epoch: 147, loss: 5.89\n",
            "epoch: 148, loss: 5.9\n",
            "epoch: 149, loss: 5.9\n",
            "epoch: 150, loss: 5.9\n",
            "epoch: 151, loss: 5.9\n",
            "epoch: 152, loss: 5.9\n",
            "epoch: 153, loss: 5.91\n",
            "epoch: 154, loss: 5.91\n",
            "epoch: 155, loss: 5.89\n",
            "epoch: 156, loss: 5.89\n",
            "epoch: 157, loss: 5.89\n",
            "epoch: 158, loss: 5.89\n",
            "epoch: 159, loss: 5.88\n",
            "epoch: 160, loss: 5.89\n",
            "epoch: 161, loss: 5.9\n",
            "epoch: 162, loss: 5.9\n",
            "epoch: 163, loss: 5.91\n",
            "epoch: 164, loss: 5.9\n",
            "epoch: 165, loss: 5.89\n",
            "epoch: 166, loss: 5.89\n",
            "epoch: 167, loss: 5.91\n",
            "epoch: 168, loss: 5.91\n",
            "epoch: 169, loss: 5.89\n",
            "epoch: 170, loss: 5.91\n",
            "epoch: 171, loss: 5.89\n",
            "epoch: 172, loss: 5.92\n",
            "epoch: 173, loss: 5.88\n",
            "epoch: 174, loss: 5.88\n",
            "epoch: 175, loss: 5.88\n",
            "epoch: 176, loss: 5.89\n",
            "epoch: 177, loss: 5.9\n",
            "epoch: 178, loss: 5.88\n",
            "epoch: 179, loss: 5.88\n",
            "epoch: 180, loss: 5.89\n",
            "epoch: 181, loss: 5.88\n",
            "epoch: 182, loss: 5.88\n",
            "epoch: 183, loss: 5.89\n",
            "epoch: 184, loss: 5.91\n",
            "epoch: 185, loss: 5.89\n",
            "epoch: 186, loss: 5.9\n",
            "epoch: 187, loss: 5.89\n",
            "epoch: 188, loss: 5.88\n",
            "epoch: 189, loss: 5.87\n",
            "epoch: 190, loss: 5.88\n",
            "epoch: 191, loss: 5.89\n",
            "epoch: 192, loss: 5.9\n",
            "epoch: 193, loss: 5.91\n",
            "epoch: 194, loss: 5.88\n",
            "epoch: 195, loss: 5.89\n",
            "epoch: 196, loss: 5.88\n",
            "epoch: 197, loss: 5.89\n",
            "epoch: 198, loss: 5.91\n",
            "epoch: 199, loss: 5.88\n",
            "epoch: 200, loss: 5.9\n",
            "epoch: 201, loss: 5.89\n",
            "epoch: 202, loss: 5.89\n",
            "epoch: 203, loss: 5.89\n",
            "epoch: 204, loss: 5.9\n",
            "epoch: 205, loss: 5.89\n",
            "epoch: 206, loss: 5.88\n",
            "epoch: 207, loss: 5.89\n",
            "epoch: 208, loss: 5.9\n",
            "epoch: 209, loss: 5.91\n",
            "epoch: 210, loss: 5.88\n",
            "epoch: 211, loss: 5.89\n",
            "epoch: 212, loss: 5.89\n",
            "epoch: 213, loss: 5.88\n",
            "epoch: 214, loss: 5.87\n",
            "epoch: 215, loss: 5.89\n",
            "epoch: 216, loss: 5.88\n",
            "epoch: 217, loss: 5.89\n",
            "epoch: 218, loss: 5.87\n",
            "epoch: 219, loss: 5.88\n",
            "epoch: 220, loss: 5.9\n",
            "epoch: 221, loss: 5.88\n",
            "epoch: 222, loss: 5.89\n",
            "epoch: 223, loss: 5.86\n",
            "epoch: 224, loss: 5.89\n",
            "epoch: 225, loss: 5.87\n",
            "epoch: 226, loss: 5.87\n",
            "epoch: 227, loss: 5.89\n",
            "epoch: 228, loss: 5.88\n",
            "epoch: 229, loss: 5.88\n",
            "epoch: 230, loss: 5.88\n",
            "epoch: 231, loss: 5.88\n",
            "epoch: 232, loss: 5.88\n",
            "epoch: 233, loss: 5.87\n",
            "epoch: 234, loss: 5.91\n",
            "epoch: 235, loss: 5.88\n",
            "epoch: 236, loss: 5.87\n",
            "epoch: 237, loss: 5.86\n",
            "epoch: 238, loss: 5.89\n",
            "epoch: 239, loss: 5.9\n",
            "epoch: 240, loss: 5.87\n",
            "epoch: 241, loss: 5.89\n",
            "epoch: 242, loss: 5.91\n",
            "epoch: 243, loss: 5.88\n",
            "epoch: 244, loss: 5.86\n",
            "epoch: 245, loss: 5.88\n",
            "epoch: 246, loss: 5.9\n",
            "epoch: 247, loss: 5.88\n",
            "epoch: 248, loss: 5.86\n",
            "epoch: 249, loss: 5.89\n",
            "epoch: 250, loss: 5.89\n",
            "epoch: 251, loss: 5.86\n",
            "epoch: 252, loss: 5.87\n",
            "epoch: 253, loss: 5.88\n",
            "epoch: 254, loss: 5.87\n",
            "epoch: 255, loss: 5.88\n",
            "epoch: 256, loss: 5.86\n",
            "epoch: 257, loss: 5.87\n",
            "epoch: 258, loss: 5.87\n",
            "epoch: 259, loss: 5.87\n",
            "epoch: 260, loss: 5.87\n",
            "epoch: 261, loss: 5.87\n",
            "epoch: 262, loss: 5.89\n",
            "epoch: 263, loss: 5.87\n",
            "epoch: 264, loss: 5.88\n",
            "epoch: 265, loss: 5.88\n",
            "epoch: 266, loss: 5.89\n",
            "epoch: 267, loss: 5.86\n",
            "epoch: 268, loss: 5.87\n",
            "epoch: 269, loss: 5.87\n",
            "epoch: 270, loss: 5.87\n",
            "epoch: 271, loss: 5.87\n",
            "epoch: 272, loss: 5.87\n",
            "epoch: 273, loss: 5.86\n",
            "epoch: 274, loss: 5.86\n",
            "epoch: 275, loss: 5.89\n",
            "epoch: 276, loss: 5.87\n",
            "epoch: 277, loss: 5.86\n",
            "epoch: 278, loss: 5.85\n",
            "epoch: 279, loss: 5.85\n",
            "epoch: 280, loss: 5.85\n",
            "epoch: 281, loss: 5.86\n",
            "epoch: 282, loss: 5.86\n",
            "epoch: 283, loss: 5.85\n",
            "epoch: 284, loss: 5.87\n",
            "epoch: 285, loss: 5.87\n",
            "epoch: 286, loss: 5.87\n",
            "epoch: 287, loss: 5.92\n",
            "epoch: 288, loss: 5.86\n",
            "epoch: 289, loss: 5.86\n",
            "epoch: 290, loss: 5.86\n",
            "epoch: 291, loss: 5.86\n",
            "epoch: 292, loss: 5.87\n",
            "epoch: 293, loss: 5.86\n",
            "epoch: 294, loss: 5.87\n",
            "epoch: 295, loss: 5.85\n",
            "epoch: 296, loss: 5.86\n",
            "epoch: 297, loss: 5.86\n",
            "epoch: 298, loss: 5.85\n",
            "epoch: 299, loss: 5.87\n",
            "epoch: 300, loss: 5.87\n",
            "0.2758096754550934 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VFJzFhBlkjv",
        "outputId": "22ecc658-57b7-4727-9e61-a64c152afc0e"
      },
      "source": [
        "\"\"\"\n",
        "y_trueが最小値を取る値を調べる\n",
        "\"\"\"\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad = True)\n",
        "y_true = 3*x**2+7\n",
        "op = torch.optim.SGD([x], lr=0.1)\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "for i in range(epochs):\n",
        "  op.zero_grad()\n",
        "  y_true.backward() #これが勝手に、y=6*x(y_trueの微分した値)を作ってくれている。\n",
        "  op.step()\n",
        "  print(\"微分した値\",x) #←この値が最小の時、y2は０に近く。最小値となる。\n",
        "  y_true = 3*x**2+7\n",
        "  print(\"求めたい値:\",y_true)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "微分した値 tensor(1.2000, requires_grad=True)\n",
            "求めたい値: tensor(11.3200, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.4800, requires_grad=True)\n",
            "求めたい値: tensor(7.6912, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.1920, requires_grad=True)\n",
            "求めたい値: tensor(7.1106, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0768, requires_grad=True)\n",
            "求めたい値: tensor(7.0177, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0307, requires_grad=True)\n",
            "求めたい値: tensor(7.0028, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0123, requires_grad=True)\n",
            "求めたい値: tensor(7.0005, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0049, requires_grad=True)\n",
            "求めたい値: tensor(7.0001, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0020, requires_grad=True)\n",
            "求めたい値: tensor(7.0000, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0008, requires_grad=True)\n",
            "求めたい値: tensor(7.0000, grad_fn=<AddBackward0>)\n",
            "微分した値 tensor(0.0003, requires_grad=True)\n",
            "求めたい値: tensor(7.0000, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDNiKctN52fS"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR7hpoVevhcK"
      },
      "source": [
        " loss = compute_loss(t,preds) #これが最小になった時、いいw,bが手に入る。\n",
        "  optimizer.zero_grad() #勾配の初期化 :pytorchでは、勾配が勝手に足されていくので、それを避けるため。前回の分が計算に入っちゃう。\n",
        "  loss.backward() #損失関数E=1/N*tlogy +(1-t)*log(1-y)のx,tについて微分を計算。\n",
        "  optimizer.step() #w←de/dwの計算\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4YcFYEO6DJW"
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoSOXu6FnbYQ"
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    }
  ]
}